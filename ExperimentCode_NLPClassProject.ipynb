{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "735ac16e-4bf3-4e47-aa7e-ce193a10ee4f",
      "metadata": {
        "id": "735ac16e-4bf3-4e47-aa7e-ce193a10ee4f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install torch\n",
        "!{sys.executable} -m pip install transformers\n",
        "!{sys.executable} -m pip install datasets\n",
        "!{sys.executable} -m pip install nltk\n",
        "!{sys.executable} -m pip install rouge\n",
        "!{sys.executable} -m pip install deep_translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SDLsYHNlw2Ss",
      "metadata": {
        "id": "SDLsYHNlw2Ss"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from rouge import Rouge\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "from deep_translator import GoogleTranslator\n",
        "nltk.download('punkt')\n",
        "\n",
        "DEVICE = torch.device(\"cuda\")\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6198a44",
      "metadata": {
        "id": "e6198a44"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset for German to English translation\n",
        "dataset = load_dataset(\"kaitchup/opus-German-to-English\")\n",
        "model_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f364894",
      "metadata": {
        "id": "1f364894"
      },
      "outputs": [],
      "source": [
        "#Functions\n",
        "\n",
        "#Add noise from N(mean, std_dev) distribution\n",
        "def add_noise(encoder_outputs, std_dev=0.1):\n",
        "    encoder_output_return = encoder_outputs\n",
        "    encoder_hidden_states = encoder_output_return.last_hidden_state\n",
        "    noise = torch.randn(encoder_hidden_states.size()) * std_dev + 0 #standard deviation = std_dev but mean is always 0, +0 for illustrative purpose\n",
        "    noisy_hidden_states = encoder_hidden_states + noise.to(encoder_hidden_states.device)\n",
        "    encoder_output_return.last_hidden_state = noisy_hidden_states\n",
        "    return encoder_output_return\n",
        "\n",
        "#Determine Bleu Score\n",
        "def bleu_score_calc(reference, candidate):\n",
        "    reference_tokens = word_tokenize(reference.lower())\n",
        "    candidate_tokens = word_tokenize(candidate.lower())\n",
        "    length = len(reference_tokens)\n",
        "    if length <= 3:\n",
        "        weights = [1]\n",
        "    elif length <= 8:\n",
        "        weights = [0.5, 0.5]\n",
        "    else: #length <= 15:\n",
        "        weights = [0.33, 0.33, 0.33]\n",
        "    return sentence_bleu([reference_tokens], candidate_tokens, weights = weights)\n",
        "\n",
        "#Determine Rouge Score\n",
        "def rouge_score_calc(reference, candidate):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(candidate, reference)\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df101d1-a749-451f-93ec-61556cd98ca0",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8df101d1-a749-451f-93ec-61556cd98ca0"
      },
      "outputs": [],
      "source": [
        "# Experiment 1: Evaluate the effect of noise on translation accuracy using a range of noise levels.\n",
        "# This experiment involves translating texts, adding noise to the translations, and then evaluating the impact using BLEU and ROUGE scores.\n",
        "\n",
        "file_path = '/content/NLP Class Project Data - OfficialInputData.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# List to store results from each noise level trial\n",
        "big_data = []\n",
        "\n",
        "# Define the standard deviations for noise to be tested\n",
        "std_dev_values = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5]\n",
        "\n",
        "# Loop through each standard deviation value to test its impact\n",
        "for std_dev in std_dev_values:\n",
        "    print(\"TRIAL: \", std_dev)\n",
        "    print(\"_________________________________________________________________________\")\n",
        "    # Initialize a dictionary to store results for this trial\n",
        "    data = {\n",
        "        'r1_r': [],\n",
        "        'r1_p': [],\n",
        "        'r1_f': [],\n",
        "        'r2_r': [],\n",
        "        'r2_p': [],\n",
        "        'r2_f': [],\n",
        "        'rl_r': [],\n",
        "        'rl_p': [],\n",
        "        'rl_f': [],\n",
        "        'bleu_scores': [],\n",
        "        'normal_text': [],\n",
        "        'noise_text': []\n",
        "    }\n",
        "    # Process each sentence in the dataset\n",
        "    for i in range(len(df)):\n",
        "        print(i)\n",
        "        input_text = df['German Google Translate (input)'][i]\n",
        "        # Tokenize the input text\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "        # Get encoder outputs\n",
        "        encoder_outputs = model.get_encoder()(input_ids=inputs['input_ids'])\n",
        "\n",
        "        # Generate translations from clean encoder outputs\n",
        "        generated_ids_normal = model.generate(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            encoder_outputs=encoder_outputs\n",
        "        )\n",
        "\n",
        "        # Add noise to the encoder outputs and generate translations from noisy outputs\n",
        "        noisy_encoder_outputs = add_noise(encoder_outputs, std_dev)\n",
        "\n",
        "        generated_ids_noise = model.generate(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            encoder_outputs=noisy_encoder_outputs\n",
        "        )\n",
        "\n",
        "        # Decode translations to text\n",
        "        normal_decoded_text = tokenizer.decode(generated_ids_normal[0], skip_special_tokens=True)\n",
        "        noise_decoded_text = tokenizer.decode(generated_ids_noise[0], skip_special_tokens=True)\n",
        "        # Calculate BLEU and ROUGE scores\n",
        "        bleu_score = bleu_score_calc(normal_decoded_text, noise_decoded_text)\n",
        "        rouge_score = rouge_score_calc(normal_decoded_text, noise_decoded_text)\n",
        "\n",
        "        # Append results to data dictionary\n",
        "        data['normal_text'].append(normal_decoded_text)\n",
        "        data['noise_text'].append(noise_decoded_text)\n",
        "        data['bleu_scores'].append(bleu_score)\n",
        "        data['r1_r'].append(rouge_score[0]['rouge-1']['r'])\n",
        "        data['r1_p'].append(rouge_score[0]['rouge-1']['p'])\n",
        "        data['r1_f'].append(rouge_score[0]['rouge-1']['f'])\n",
        "        data['r2_r'].append(rouge_score[0]['rouge-2']['r'])\n",
        "        data['r2_p'].append(rouge_score[0]['rouge-2']['p'])\n",
        "        data['r2_f'].append(rouge_score[0]['rouge-2']['f'])\n",
        "        data['rl_r'].append(rouge_score[0]['rouge-l']['r'])\n",
        "        data['rl_p'].append(rouge_score[0]['rouge-l']['p'])\n",
        "        data['rl_f'].append(rouge_score[0]['rouge-l']['f'])\n",
        "    # Store results for this trial in the main list\n",
        "    big_data.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jeTpuePrK8In",
      "metadata": {
        "id": "jeTpuePrK8In"
      },
      "outputs": [],
      "source": [
        "# Export Data to Excel File For Plotting and Analysis\n",
        "dfs = {key: pd.DataFrame(value) for key, value in enumerate(big_data)}\n",
        "\n",
        "with pd.ExcelWriter('output_experiment1_trial#.xlsx') as writer:\n",
        "    for sheet_name, df in dfs.items():\n",
        "        df.to_excel(writer, sheet_name=str(sheet_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eabc6065-4972-4003-9f5e-73a46241b28a",
      "metadata": {
        "id": "eabc6065-4972-4003-9f5e-73a46241b28a"
      },
      "outputs": [],
      "source": [
        "# Experiment 2: Analyze the preservation of semantic information by comparing noisy and clean embeddings in the embedding space.\n",
        "# Take the embeddings from the original English phrases converted to German, add noise to these embeddings,\n",
        "# decode them into English, translate them back to German, embed them again, and find cosign similarity between clean and noisy embeddings\n",
        "# embedding space\n",
        "\n",
        "file_path = '/content/NLP Class Project Data - OfficialInputData.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# List to store results from each noise level trial\n",
        "big_data2 = []\n",
        "std_dev_values = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5]\n",
        "\n",
        "# Iterate over each standard deviation value to test its impact\n",
        "for std_dev in std_dev_values:\n",
        "    print(\"TRIAL: \", std_dev)\n",
        "    print(\"_________________________________________________________________________\")\n",
        "    # Dictionary to store results for this trial\n",
        "    data = {\n",
        "        'similarities': [],\n",
        "        'similarities_squared': [],\n",
        "        'original_english': [],\n",
        "        'noisy_english': [],\n",
        "        'noisy_german_translated': []\n",
        "    }\n",
        "    # Process each sentence in the dataset\n",
        "    for i in range(len(df)):\n",
        "        print(i)\n",
        "        input_text = df['German Google Translate (input)'][i]\n",
        "        # Tokenize the input text\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "        # Get encoder outputs\n",
        "        encoder_outputs = model.get_encoder()(input_ids=inputs['input_ids'])\n",
        "        # Clone the encoder outputs to preserve original embeddings for comparison\n",
        "        saved_encoder_outputs = {attr: getattr(encoder_outputs, attr).clone() if isinstance(getattr(encoder_outputs, attr), torch.Tensor) else getattr(encoder_outputs, attr) for attr in encoder_outputs.__annotations__.keys()}\n",
        "\n",
        "        # Add noise to encoder outputs\n",
        "        noisy_encoder_outputs = add_noise(encoder_outputs, std_dev)\n",
        "\n",
        "        # Generate translations from noisy encoder outputs\n",
        "        generated_ids_noise = model.generate(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            encoder_outputs=noisy_encoder_outputs\n",
        "        )\n",
        "\n",
        "        # Decode noisy translations to text\n",
        "        noise_decoded_text = tokenizer.decode(generated_ids_noise[0], skip_special_tokens=True)\n",
        "        # Translate noisy English text back to German using Google Translate\n",
        "        translated_noisy = GoogleTranslator(source='en', target='de').translate(noise_decoded_text)\n",
        "\n",
        "        # Tokenize and re-encode the translated noisy German text\n",
        "        inputs_noise = tokenizer(translated_noisy, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        noise_encoded = model.get_encoder()(input_ids=inputs_noise['input_ids'])\n",
        "\n",
        "        # Calculate the mean of the embeddings for both original and noisy texts - mean reduction\n",
        "        noise_tensor = noise_encoded.last_hidden_state\n",
        "        original_tensor = saved_encoder_outputs['last_hidden_state']\n",
        "        original_vector = original_tensor.mean(dim=1)\n",
        "        noise_vector = noise_tensor.mean(dim=1)\n",
        "\n",
        "        # Compute cosine similarity and squared cosine similarity between original and noisy embeddings\n",
        "        similarity = F.cosine_similarity(original_vector, noise_vector, dim=1)\n",
        "        similarity_squared = similarity ** 2\n",
        "\n",
        "        # Store results in dictionary\n",
        "        data['original_english'].append(df['English (gold output)'][i])\n",
        "        data['noisy_english'].append(noise_decoded_text)\n",
        "        data['noisy_german_translated'].append(translated_noisy)\n",
        "        data['similarities'].append(similarity)\n",
        "        data['similarities_squared'].append(similarity_squared)\n",
        "    # Append trial data to the main list\n",
        "    big_data2.append(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QcmUPRtVeqbu",
      "metadata": {
        "id": "QcmUPRtVeqbu"
      },
      "outputs": [],
      "source": [
        "# Export Data to Excel File For Plotting and Analysis\n",
        "dfs = {key: pd.DataFrame(value) for key, value in enumerate(big_data2)}\n",
        "\n",
        "with pd.ExcelWriter('output_experiment2_trial#.xlsx') as writer:\n",
        "    for sheet_name, df in dfs.items():\n",
        "        df.to_excel(writer, sheet_name=str(sheet_name))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}